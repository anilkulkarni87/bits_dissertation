{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "674460ce-63bb-4cf9-938c-91360e30abbb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import current_timestamp, col\n",
    "\n",
    "df_raw = spark.table(\"bits_pilani.bronze_sch.synthetic_retail_transactions\")  \n",
    "\n",
    "# Drop the existing table if it exists\n",
    "spark.sql(\"DROP TABLE IF EXISTS bits_pilani.bronze_sch.retail_transactions_raw\")\n",
    "\n",
    "df_raw.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"bits_pilani.bronze_sch.retail_transactions_raw\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "03c55412-a7fd-4353-bdd6-dd666dc14939",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import to_date, when, col\n",
    "\n",
    "df_bronze = spark.table(\"bits_pilani.bronze_sch.retail_transactions_raw\")\n",
    "\n",
    "df_silver = (\n",
    "    df_bronze.withColumn(\"TXN_DATE\", to_date(\"TXN_TIMESTAMP\"))\n",
    "             .withColumn(\"QUANTITY\", col(\"QUANTITY\").cast(\"int\"))\n",
    "             .withColumn(\"UNIT_REGULAR_PRICE_USD\", col(\"UNIT_REGULAR_PRICE_USD\").cast(\"double\"))\n",
    "             .withColumn(\"EXTENDED_PRICE_USD\", col(\"EXTENDED_PRICE_USD\").cast(\"double\"))\n",
    "             .withColumn(\"GROSS_MARGIN_AMT_USD\", col(\"GROSS_MARGIN_AMT_USD\").cast(\"double\"))\n",
    "             .withColumn(\"GUEST_TYPE\", when(col(\"GUEST_ID\").isNull(), \"ANONYMOUS\").otherwise(\"IDENTIFIED\"))\n",
    ")\n",
    "\n",
    "# Drop the existing table if it exists\n",
    "spark.sql(\"DROP TABLE IF EXISTS bits_pilani.silver_sch.retail_transactions_clean\")\n",
    "\n",
    "df_silver.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"bits_pilani.silver_sch.retail_transactions_clean\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dec32eb5-26e8-4de3-ab2f-c464268801bc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import (\n",
    "    col, count, sum as _sum, avg, max as _max, datediff, lit,\n",
    "    when, concat_ws, lower, row_number\n",
    ")\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "# Load Silver data\n",
    "df_silver = spark.table(\"bits_pilani.silver_sch.retail_transactions_clean\")\n",
    "linked_df = df_silver.filter(col(\"MASTER_GUEST_ID\").isNotNull())\n",
    "\n",
    "# Reference date\n",
    "ref_date = linked_df.agg(_max(\"TXN_TIMESTAMP\").alias(\"max_ts\")).collect()[0][\"max_ts\"]\n",
    "\n",
    "# ========== STEP 1: Transaction Count per Channel ==========\n",
    "channel_counts_df = linked_df.withColumn(\"txn_STORE\", when(col(\"MASTER_ORDER_ORIGIN\") == \"STORE\", 1).otherwise(0)) \\\n",
    "                             .withColumn(\"txn_ECOM\",  when(col(\"MASTER_ORDER_ORIGIN\") == \"ECOM\",  1).otherwise(0)) \\\n",
    "                             .withColumn(\"txn_APP\",   when(col(\"MASTER_ORDER_ORIGIN\") == \"APP\",   1).otherwise(0))\n",
    "\n",
    "channel_summary = channel_counts_df.groupBy(\"MASTER_GUEST_ID\").agg(\n",
    "    _sum(\"txn_STORE\").alias(\"store_txns\"),\n",
    "    _sum(\"txn_ECOM\").alias(\"ecom_txns\"),\n",
    "    _sum(\"txn_APP\").alias(\"app_txns\")\n",
    ")\n",
    "\n",
    "# ========== STEP 2: Top Category using Window ==========\n",
    "dept_counts = linked_df.groupBy(\"MASTER_GUEST_ID\", \"MMS_DEPT_NAME\") \\\n",
    "                       .agg(count(\"*\").alias(\"dept_count\"))\n",
    "\n",
    "dept_window = Window.partitionBy(\"MASTER_GUEST_ID\").orderBy(col(\"dept_count\").desc())\n",
    "\n",
    "top_dept = dept_counts.withColumn(\"rank\", row_number().over(dept_window)) \\\n",
    "                      .filter(col(\"rank\") == 1) \\\n",
    "                      .select(\"MASTER_GUEST_ID\", col(\"MMS_DEPT_NAME\").alias(\"top_category\"))\n",
    "\n",
    "# ========== STEP 3: Main Aggregation ==========\n",
    "summary = linked_df.groupBy(\"MASTER_GUEST_ID\").agg(\n",
    "    count(\"TRANSACTION_ID\").alias(\"total_transactions\"),\n",
    "    _sum(\"EXTENDED_PRICE_USD\").alias(\"total_spend_usd\"),\n",
    "    avg(\"DISCOUNT_PCT\").alias(\"avg_discount_pct\"),\n",
    "    _max(\"TXN_TIMESTAMP\").alias(\"last_txn_date\"),\n",
    "    expr(\"mode() within group (order by MASTER_ORDER_ORIGIN)\").alias(\"top_channel\"),\n",
    "    expr(\"mode() within group (order by SELLING_MARKET)\").alias(\"top_market\"),\n",
    "    expr(\"mode() within group (order by COUPON_CODE)\").alias(\"top_coupon\")\n",
    ").withColumn(\n",
    "    \"days_since_last_txn\", datediff(lit(ref_date), col(\"last_txn_date\"))\n",
    ")\n",
    "\n",
    "# ========== STEP 4: Join Enrichments ==========\n",
    "summary = summary.join(top_dept, on=\"MASTER_GUEST_ID\", how=\"left\") \\\n",
    "                 .join(channel_summary, on=\"MASTER_GUEST_ID\", how=\"left\")\n",
    "\n",
    "# ========== STEP 5: Persona Columns ==========\n",
    "summary = summary \\\n",
    "    .withColumn(\"frequency\",\n",
    "        when(col(\"total_transactions\") >= 10, \"frequent\")\n",
    "        .when(col(\"total_transactions\") >= 5, \"regular\")\n",
    "        .otherwise(\"occasional\")\n",
    "    ).withColumn(\"spender\",\n",
    "        when(col(\"total_spend_usd\") >= 500, \"high spender\")\n",
    "        .when(col(\"total_spend_usd\") >= 200, \"mid-level spender\")\n",
    "        .otherwise(\"low spender\")\n",
    "    ).withColumn(\"deal_type\",\n",
    "        when(col(\"avg_discount_pct\") >= 20, \"deal-seeker\")\n",
    "        .when(col(\"avg_discount_pct\") >= 5, \"value-conscious\")\n",
    "        .otherwise(\"full-price shopper\")\n",
    "    ).withColumn(\"recency\",\n",
    "        when(col(\"days_since_last_txn\") <= 30, \"recent\")\n",
    "        .when(col(\"days_since_last_txn\") <= 90, \"somewhat active\")\n",
    "        .otherwise(\"lapsed\")\n",
    "    ).withColumn(\"coupon_phrase\",\n",
    "        when(col(\"top_coupon\").isNotNull(),\n",
    "             concat_ws(\" \", lit(\"frequently uses promo codes like\"), col(\"top_coupon\")))\n",
    "        .otherwise(lit(\"rarely uses promotions\"))\n",
    "    ).withColumn(\"persona_sentence\",\n",
    "        concat_ws(\" \",\n",
    "            col(\"frequency\"),\n",
    "            lower(col(\"top_channel\")), lit(\"shopper from\"),\n",
    "            col(\"top_market\"), lit(\"focused on\"),\n",
    "            concat_ws(\"\", col(\"top_category\"), lit(\", a\")),\n",
    "            col(\"spender\"), lit(\"and\"), concat_ws(\"\", col(\"deal_type\"), lit(\",\")),\n",
    "            col(\"recency\"), lit(\"guest who\"), concat_ws(\"\", col(\"coupon_phrase\"), lit(\",\")),\n",
    "            lit(\"last purchased\"), col(\"days_since_last_txn\").cast(\"string\"), lit(\"days ago.\"),\n",
    "            lit(\"Has\"), col(\"store_txns\").cast(\"string\"), lit(\"STORE,\"),\n",
    "            col(\"ecom_txns\").cast(\"string\"), lit(\"ECOM,\"),\n",
    "            col(\"app_txns\").cast(\"string\"), lit(\"APP transactions.\")\n",
    "        )\n",
    "    )\n",
    "\n",
    "# ========== STEP 6: Write to Gold Layer ==========\n",
    "spark.sql(\"DROP TABLE IF EXISTS bits_pilani.gold_sch.guest_persona_summary\")\n",
    "\n",
    "summary.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"bits_pilani.gold_sch.guest_persona_summary\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f8ba632c-8156-4b40-98ec-30cb19e7e6aa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "!pip install -U kaleido"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7f4211bc-7928-41b4-ae4f-c572f6a30067",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ðŸ“˜ guest_persona_clustering.py (Restricted Cluster Version with Labels & Visualization)\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.express as px\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import pandas as pd\n",
    "\n",
    "# Step 1: Load Gold Summary Table\n",
    "df_spark = spark.table(\"bits_pilani.gold_sch.guest_persona_summary\")\n",
    "\n",
    "# Step 2: Convert to Pandas for local ML\n",
    "pdf = df_spark.select(\n",
    "    \"MASTER_GUEST_ID\", \"total_transactions\", \"total_spend_usd\", \"avg_discount_pct\",\"top_category\",\n",
    "    \"days_since_last_txn\", \"store_txns\", \"ecom_txns\", \"app_txns\", \"persona_sentence\"\n",
    ").toPandas()\n",
    "\n",
    "# Step 3: Manual One-Hot Encoding for 'top_channel'\n",
    "# pdf = pd.get_dummies(pdf, columns=[\"top_channel\"], prefix=\"channel\")\n",
    "\n",
    "# Step 4: Fill any missing values\n",
    "pdf.fillna(0, inplace=True)\n",
    "\n",
    "# Step 5: Standardize Features\n",
    "features = [\n",
    "    \"total_transactions\", \"total_spend_usd\", \"avg_discount_pct\",\n",
    "    \"days_since_last_txn\", \"store_txns\", \"ecom_txns\", \"app_txns\"\n",
    "]\n",
    "scaler = StandardScaler()\n",
    "pdf_scaled = scaler.fit_transform(pdf[features])\n",
    "\n",
    "# Step 6: PCA\n",
    "pca = PCA(n_components=2)\n",
    "pca_result = pca.fit_transform(pdf_scaled)\n",
    "pdf[\"pca_x\"] = pca_result[:, 0]\n",
    "pdf[\"pca_y\"] = pca_result[:, 1]\n",
    "\n",
    "# Step 7: KMeans Clustering\n",
    "kmeans = KMeans(n_clusters=5, random_state=42)\n",
    "pdf[\"persona_cluster\"] = kmeans.fit_predict(pca_result)\n",
    "\n",
    "# Step 8: Summarize clusters for labeling\n",
    "cluster_summary = pdf.groupby(\"persona_cluster\").agg({\n",
    "    \"total_spend_usd\": \"mean\",\n",
    "    \"total_transactions\": \"mean\",\n",
    "    \"avg_discount_pct\": \"mean\",\n",
    "    \"days_since_last_txn\": \"mean\",\n",
    "    \"store_txns\": \"mean\",\n",
    "    \"ecom_txns\": \"mean\",\n",
    "    \"app_txns\": \"mean\"\n",
    "}).reset_index()\n",
    "display(cluster_summary)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "21ce5ca4-2575-47af-ba0e-4ae8c5df3faf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def generate_cluster_sentence(row):\n",
    "    # Spend category\n",
    "    if row['total_spend_usd'] >= 1500:\n",
    "        spender = \"high spenders\"\n",
    "    elif row['total_spend_usd'] >= 1000:\n",
    "        spender = \"mid spenders\"\n",
    "    else:\n",
    "        spender = \"low spenders\"\n",
    "\n",
    "    # Engagement\n",
    "    engagement = \"highly engaged\" if row['total_transactions'] > 10 else \"moderately engaged\"\n",
    "\n",
    "    # Recency\n",
    "    if row['days_since_last_txn'] <= 30:\n",
    "        recency = \"recently active\"\n",
    "    elif row['days_since_last_txn'] <= 60:\n",
    "        recency = \"somewhat active\"\n",
    "    else:\n",
    "        recency = \"lapsed\"\n",
    "\n",
    "    # Channel preference\n",
    "    top_channel = max(\n",
    "        [\"store_txns\", \"ecom_txns\", \"app_txns\"],\n",
    "        key=lambda c: row[c]\n",
    "    ).replace(\"_txns\", \"\").upper()\n",
    "\n",
    "    # Deal type\n",
    "    if row['avg_discount_pct'] > 8:\n",
    "        deal_type = \"deal seekers\"\n",
    "    elif row['avg_discount_pct'] > 4:\n",
    "        deal_type = \"value-conscious\"\n",
    "    else:\n",
    "        deal_type = \"full-price shoppers\"\n",
    "\n",
    "    return (\n",
    "        f\"These are {spender} who are {engagement} and {recency}. \"\n",
    "        f\"They primarily shop via {top_channel}, and are typically {deal_type}.\"\n",
    "    )\n",
    "\n",
    "cluster_summary[\"cluster_sentence\"] = cluster_summary.apply(generate_cluster_sentence, axis=1)\n",
    "display(cluster_summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8414acbe-c65f-49db-b373-e60a3646f8a5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "| Cluster | Label                                  | Reasoning                                                             |\n",
    "| ------: | -------------------------------------- | --------------------------------------------------------------------- |\n",
    "|   **0** | **Mid-Spend Omnichannel Guests**       | Moderate spend (\\$1153), balanced use of store/ecom/app               |\n",
    "|   **1** | **High-Spend Multi-Channel Loyalists** | Highest spend (\\$1921), highest transaction count, low recency        |\n",
    "|   **2** | **High-Spend Deal Seekers**            | High spend (\\$1359), high discount usage (8.6%), high app + ecom mix  |\n",
    "|   **3** | **Low-Spend Store Shoppers**           | Lower spend (\\$799), mostly in-store, longer recency                  |\n",
    "|   **4** | **Lapsed Low-Spend Guests**            | Lowest spend (\\$572), low frequency, lapsed (98+ days), least engaged |\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### ðŸ§  How These Labels Were Decided\n",
    "\n",
    "Labels were assigned based on the following cluster-level patterns:\n",
    "\n",
    "#### ðŸ’° Spending Tiers\n",
    "- **\\$1500+** â†’ High spender  \n",
    "- **\\$1000â€“1500** â†’ Mid spender  \n",
    "- **< \\$1000** â†’ Low spender  \n",
    "\n",
    "#### ðŸ”„ Engagement\n",
    "- **`total_transactions` > 10** â†’ Highly engaged  \n",
    "- **`days_since_last_txn` > 60** â†’ Lapsed or inactive  \n",
    "\n",
    "#### ðŸ›ï¸ Channel Preference\n",
    "- Determined based on the dominant channel among:\n",
    "  - `store_txns`\n",
    "  - `ecom_txns`\n",
    "  - `app_txns`\n",
    "\n",
    "#### ðŸŽ¯ Discount Sensitivity\n",
    "- **`avg_discount_pct` > 8** â†’ Deal seeker  \n",
    "- Otherwise â†’ Full-price or value-conscious shopper\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1b8ebc5f-e99f-4ead-8837-a436d237f5ea",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Step 9: Manual label assignment\n",
    "cluster_labels = {\n",
    "    0: \"Mid-Spend Omnichannel Guests\",\n",
    "    1: \"High-Spend Multi-Channel Loyalists\",\n",
    "    2: \"High-Spend Deal Seekers\",\n",
    "    3: \"Low-Spend Store Shoppers\",\n",
    "    4: \"Lapsed Low-Spend Guests\"\n",
    "}\n",
    "# Add label column to cluster_summary\n",
    "cluster_summary[\"persona_cluster_label\"] = cluster_summary[\"persona_cluster\"].map(cluster_labels)\n",
    "\n",
    "# Step 10: Map cluster labels to the DataFrame\n",
    "pdf[\"persona_cluster_label\"] = pdf[\"persona_cluster\"].map(cluster_labels)\n",
    "\n",
    "# Create a mapping\n",
    "cluster_sentence_map = dict(zip(\n",
    "    cluster_summary[\"persona_cluster_label\"],\n",
    "    cluster_summary[\"cluster_sentence\"]\n",
    "))\n",
    "\n",
    "# Add to guest-level pdf\n",
    "pdf[\"cluster_sentence\"] = pdf[\"persona_cluster_label\"].map(cluster_sentence_map)\n",
    "\n",
    "# Optional: Combine with persona sentence\n",
    "pdf[\"final_persona_sentence\"] = pdf[\"persona_sentence\"] + \" \" + pdf[\"cluster_sentence\"]\n",
    "\n",
    "\n",
    "# Step 2: Compute cluster centers and average spend\n",
    "cluster_centers = pdf.groupby(\"persona_cluster_label\").agg({\n",
    "    \"pca_x\": \"mean\",\n",
    "    \"pca_y\": \"mean\",\n",
    "    \"total_spend_usd\": \"mean\"\n",
    "}).reset_index()\n",
    "\n",
    "# Step 3: Create scatter plot with clusters\n",
    "fig = px.scatter(\n",
    "    pdf,\n",
    "    x=\"pca_x\",\n",
    "    y=\"pca_y\",\n",
    "    color=\"persona_cluster_label\",\n",
    "    title=\"ðŸ§¬ Guest Persona Clusters (PCA Projection)\",\n",
    "    hover_data=[\n",
    "        \"MASTER_GUEST_ID\",\n",
    "        \"persona_sentence\",\n",
    "        \"total_spend_usd\",\n",
    "        \"total_transactions\",\n",
    "        \"avg_discount_pct\",\n",
    "        \"days_since_last_txn\"\n",
    "    ],\n",
    "    width=950,\n",
    "    height=600\n",
    ")\n",
    "\n",
    "# Step 4: Annotate cluster centers with average spend\n",
    "for _, row in cluster_centers.iterrows():\n",
    "    fig.add_trace(go.Scatter(\n",
    "        x=[row[\"pca_x\"]],\n",
    "        y=[row[\"pca_y\"]],\n",
    "        text=[f\"{row['persona_cluster_label']}<br>Avg Spend: ${row['total_spend_usd']:.0f}\"],\n",
    "        mode=\"text\",\n",
    "        showlegend=False\n",
    "    ))\n",
    "\n",
    "fig.update_layout(legend_title_text='Persona Cluster')\n",
    "fig.show()\n",
    "\n",
    "\n",
    "# Save Plotly figure to PNG\n",
    "# fig.write_image(\"/dbfs/tmp/guest_persona_clusters.png\", width=950, height=600, scale=2)\n",
    "fig.write_html(\"/Volumes/bits_pilani/raw_sch/raw_data/guest_persona_clusters.html\")\n",
    "displayHTML(\"<a href='/Volumes/bits_pilani/raw_sch/raw_data/guest_persona_clusters.html' target='_blank'>ðŸ“¥ Open & Download Interactive Plot</a>\")\n",
    "\n",
    "\n",
    "# Provide download link\n",
    "# displayHTML(\"<a href='/files/tmp/guest_persona_clusters.png' target='_blank'>ðŸ“¥ Download PNG</a>\")\n",
    "\n",
    "\n",
    "\n",
    "# Step 11: Optionally preview final output\n",
    "pdf_final = pdf[[\n",
    "    \"MASTER_GUEST_ID\", \"persona_cluster\", \"persona_cluster_label\", \"persona_sentence\",\n",
    "    \"cluster_sentence\",\"final_persona_sentence\",\n",
    "    \"total_spend_usd\", \"total_transactions\", \"days_since_last_txn\",\"top_category\",\n",
    "    \"store_txns\", \"ecom_txns\", \"app_txns\", \"avg_discount_pct\", \"pca_x\", \"pca_y\"\n",
    "]]\n",
    "display(pdf_final.head())\n",
    "\n",
    "# Save enriched persona table (e.g., to CSV or back to Spark)\n",
    "pdf_final.to_csv(\"final_persona_profiles.csv\", index=False)\n",
    "\n",
    "# Or convert back to Spark if needed:\n",
    "df_enriched = spark.createDataFrame(pdf_final)\n",
    "df_enriched.write.mode(\"overwrite\").option(\"mergeSchema\", \"true\").saveAsTable(\"bits_pilani.gold_sch.guest_persona_enriched\")\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": "HIGH"
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "02_RTL_MEDALLION_FLOW",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
